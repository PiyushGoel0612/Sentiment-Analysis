{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Gag7fSSjRIqg"
      },
      "outputs": [],
      "source": [
        "! pip install newspaper3k lxml_html_clean fastapi &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "o8Rzpi2FhGpf"
      },
      "outputs": [],
      "source": [
        "!pip install uvicorn nest-asyncio pyngrok transformers torch newspaper3k nltk &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import time\n",
        "from newspaper import Article\n",
        "from urllib.parse import urlparse\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "import re"
      ],
      "metadata": {
        "id": "IXPC47XrEyb4"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zkZYEZHy4FfJ"
      },
      "outputs": [],
      "source": [
        "visited = set()\n",
        "results = []\n",
        "\n",
        "# https://gnews.io/dashboard\n",
        "API_KEY = \"03797cb80667beed8ea1bc74341941d5\"\n",
        "GNEWS_API_URL = \"https://gnews.io/api/v4/search\"\n",
        "\n",
        "def get_gnews_articles(topic, max_articles=5):\n",
        "    params = {\n",
        "        \"q\": topic,\n",
        "        \"lang\": \"en\",\n",
        "        \"max\": max_articles,\n",
        "        \"token\": API_KEY\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(GNEWS_API_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get(\"articles\", [])\n",
        "\n",
        "        results = []\n",
        "        for item in articles:\n",
        "            results.append({\n",
        "                'title': item.get(\"title\", \"\"),\n",
        "                'link': item.get(\"url\", \"\"),\n",
        "                'pubDate': item.get(\"publishedAt\", \"\")\n",
        "            })\n",
        "\n",
        "        print(f\"Found {len(results)} articles for topic '{topic}'\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(\"GNews API search failed:\", e)\n",
        "        return []\n",
        "\n",
        "def extract_info_with_newspaper(url):\n",
        "    try:\n",
        "        print(f\"Extracting article from {url}...\")\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "        text = article.text.strip()\n",
        "\n",
        "        if not text:\n",
        "            print(f\"No text found in article at {url}\")\n",
        "            return None\n",
        "\n",
        "        # Extract metadata\n",
        "        title = article.title or \"No Title\"\n",
        "        date = article.publish_date.strftime(\"%Y-%m-%d\") if article.publish_date else \"Unknown\"\n",
        "        source = urlparse(url).netloc\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': title,\n",
        "            'date': date,\n",
        "            'source': source,\n",
        "            'text': clean_text(text)\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Failed to extract article from {url}: {e}\")\n",
        "        return None\n",
        "\n",
        "def clean_text(text):\n",
        "    lines = text.split('\\n')\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) > 5 and not line.startswith('ADVERTISEMENT'):\n",
        "            cleaned.append(line)\n",
        "    return ' '.join(cleaned)\n",
        "\n",
        "def crawl_and_extract(url):\n",
        "    global visited, results\n",
        "\n",
        "    if url in visited:\n",
        "        return\n",
        "\n",
        "    visited.add(url)\n",
        "    info = extract_info_with_newspaper(url)\n",
        "    if info:\n",
        "        results.append(info)\n",
        "        print(f\"Crawled: {url}\")\n",
        "\n",
        "def fetch_data(topic):\n",
        "    resp = get_gnews_articles(topic, max_articles=5)\n",
        "\n",
        "    for a in resp:\n",
        "        print(f\"Processing article: {a['title']}\")\n",
        "        crawl_and_extract(a['link'])\n",
        "        print(\"=\" * 50)\n",
        "\n",
        "    print(\"Done. Crawled latest news data saved to results\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = 0\n",
        "\n",
        "# Load Hugging Face pipelines with proper error handling\n",
        "try:\n",
        "    summarizer = pipeline(\"summarization\", model=\"facebook/bart-large-cnn\", device=device)\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\", device=device)\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "    # Fallback to default models\n",
        "    summarizer = pipeline(\"summarization\", device=device)\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", device=device)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Clean text to remove problematic characters and tokens\"\"\"\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    # Remove or replace problematic characters\n",
        "    import re\n",
        "    # Remove non-printable characters except common whitespace\n",
        "    text = re.sub(r'[^\\x20-\\x7E\\n\\r\\t]', ' ', text)\n",
        "    # Replace multiple whitespace with single space\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Strip leading/trailing whitespace\n",
        "    text = text.strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "def safe_chunk_text(text, max_length=900):\n",
        "    \"\"\"Safely chunk text with character limit consideration\"\"\"\n",
        "    text = clean_text(text)\n",
        "    if not text:\n",
        "        return [\"\"]\n",
        "\n",
        "    # Use character-based chunking for better control\n",
        "    chunks = []\n",
        "    words = text.split()\n",
        "    current_chunk = []\n",
        "    current_length = 0\n",
        "\n",
        "    for word in words:\n",
        "        word_length = len(word) + 1  # +1 for space\n",
        "        if current_length + word_length > max_length and current_chunk:\n",
        "            chunks.append(' '.join(current_chunk))\n",
        "            current_chunk = [word]\n",
        "            current_length = word_length\n",
        "        else:\n",
        "            current_chunk.append(word)\n",
        "            current_length += word_length\n",
        "\n",
        "    if current_chunk:\n",
        "        chunks.append(' '.join(current_chunk))\n",
        "\n",
        "    return chunks if chunks else [\"\"]\n",
        "\n",
        "def safe_summarize(text, summarizer, max_attempts=3):\n",
        "    \"\"\"Safely summarize text with fallbacks\"\"\"\n",
        "    if not text or len(text.strip()) < 10:\n",
        "        return \"Content too short to summarize.\"\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            # Progressive length reduction on each attempt\n",
        "            max_input_length = 1000 - (attempt * 200)\n",
        "            if len(text) > max_input_length:\n",
        "                text = text[:max_input_length] + \"...\"\n",
        "\n",
        "            result = summarizer(\n",
        "                text,\n",
        "                max_length=min(120, len(text.split()) // 2),\n",
        "                min_length=min(30, len(text.split()) // 4),\n",
        "                do_sample=False,\n",
        "                truncation=True\n",
        "            )\n",
        "            return result[0]['summary_text']\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Summarization attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt == max_attempts - 1:\n",
        "                # Final fallback: return first few sentences\n",
        "                sentences = text.split('.')[:3]\n",
        "                return '. '.join(sentences) + '.' if sentences else \"Summarization failed.\"\n",
        "            continue\n",
        "\n",
        "def safe_sentiment_analysis(text, analyzer, max_attempts=3):\n",
        "    \"\"\"Safely analyze sentiment with fallbacks\"\"\"\n",
        "    if not text or len(text.strip()) < 5:\n",
        "        return {'label': 'NEUTRAL', 'score': 0.5}\n",
        "\n",
        "    for attempt in range(max_attempts):\n",
        "        try:\n",
        "            # Progressive length reduction\n",
        "            max_input_length = 400 - (attempt * 100)\n",
        "            if len(text) > max_input_length:\n",
        "                # Take from beginning and end to preserve context\n",
        "                half_length = max_input_length // 2\n",
        "                text = text[:half_length] + \" ... \" + text[-half_length:]\n",
        "\n",
        "            result = analyzer(text, truncation=True)\n",
        "            return result[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Sentiment analysis attempt {attempt + 1} failed: {e}\")\n",
        "            if attempt == max_attempts - 1:\n",
        "                return {'label': 'NEUTRAL', 'score': 0.5}\n",
        "            continue\n",
        "\n",
        "def process_articles():\n",
        "    \"\"\"Process articles with comprehensive error handling\"\"\"\n",
        "    global results\n",
        "\n",
        "    if not results:\n",
        "        print(\"No results to process\")\n",
        "        return []\n",
        "\n",
        "    temp = []\n",
        "\n",
        "    for idx, article in enumerate(results):\n",
        "        try:\n",
        "            print(f\"Processing article {idx + 1}/{len(results)}: {article.get('title', 'Unknown')[:50]}...\")\n",
        "\n",
        "            # Get and clean the text\n",
        "            article_text = article.get('text', article.get('content', ''))\n",
        "            if not article_text:\n",
        "                print(f\"  Warning: No text content found for article {idx + 1}\")\n",
        "                temp.append({\n",
        "                    'title': article.get('title', 'Unknown Title'),\n",
        "                    'url': article.get('url', ''),\n",
        "                    'summary': 'No content available for summarization.',\n",
        "                    'sentiment': {'label': 'NEUTRAL', 'score': 0.5}\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Clean the text\n",
        "            cleaned_text = clean_text(article_text)\n",
        "            if len(cleaned_text) < 20:\n",
        "                temp.append({\n",
        "                    'title': article.get('title', 'Unknown Title'),\n",
        "                    'url': article.get('url', ''),\n",
        "                    'summary': 'Content too short to process.',\n",
        "                    'sentiment': {'label': 'NEUTRAL', 'score': 0.5}\n",
        "                })\n",
        "                continue\n",
        "\n",
        "            # Chunk the text safely\n",
        "            chunks = safe_chunk_text(cleaned_text)\n",
        "\n",
        "            # Summarize each chunk\n",
        "            summaries = []\n",
        "            for i, chunk in enumerate(chunks):\n",
        "                print(f\"  Summarizing chunk {i + 1}/{len(chunks)}...\")\n",
        "                summary = safe_summarize(chunk, summarizer)\n",
        "                summaries.append(summary)\n",
        "\n",
        "            full_summary = \" \".join(summaries)\n",
        "\n",
        "            # Analyze sentiment on a safe portion of text\n",
        "            sentiment_text = cleaned_text[:300]  # Safe length for sentiment analysis\n",
        "            print(f\"  Analyzing sentiment...\")\n",
        "            sentiment = safe_sentiment_analysis(sentiment_text, sentiment_analyzer)\n",
        "\n",
        "            temp.append({\n",
        "                'title': article.get('title', 'Unknown Title'),\n",
        "                'url': article.get('url', ''),\n",
        "                'summary': full_summary,\n",
        "                'sentiment': sentiment\n",
        "            })\n",
        "\n",
        "            print(f\"  ✓ Article {idx + 1} processed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"  ✗ Error processing article {idx + 1}: {e}\")\n",
        "            # Add a fallback entry\n",
        "            temp.append({\n",
        "                'title': article.get('title', f'Article {idx + 1}'),\n",
        "                'url': article.get('url', ''),\n",
        "                'summary': f'Error processing article: {str(e)[:100]}...',\n",
        "                'sentiment': {'label': 'NEUTRAL', 'score': 0.5}\n",
        "            })\n",
        "\n",
        "    print(f\"\\nProcessing complete. Successfully processed {len(temp)} articles.\")\n",
        "    return temp"
      ],
      "metadata": {
        "id": "8WIfppeuHIJ1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e347b86-aa3b-4e47-be8c-1d5dbf48eec0"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "E6khJyIzHGVk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f573a7ea-2f49-4343-ee21-2333eab40828"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Authtoken saved to configuration file: /root/.config/ngrok/ngrok.yml\n"
          ]
        }
      ],
      "source": [
        "!ngrok config add-authtoken 2xmOnusF7Uhy7I9ChbKLH47TOzb_635DbfWmkg2yRmCY1Vmk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E-6Lmp1HSmUo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21314a96-ceeb-43ee-9d11-a7675881059d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Public URL: NgrokTunnel: \"https://mudfish-glorious-jackal.ngrok-free.app\" -> \"http://localhost:8000\"/analyze/\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:     Started server process [880]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "WARNING:pyngrok.process.ngrok:t=2025-05-30T17:16:07+0000 lvl=warn msg=\"failed to check for update\" obj=updater err=\"Post \\\"https://update.equinox.io/check\\\": context deadline exceeded\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5 articles for topic 'Politics in India'\n",
            "Processing article: Mint Primer: Can Bangladesh afford to mix politics and trade?\n",
            "==================================================\n",
            "Processing article: Parsa Venkateshwar Rao Jr | The Conundrum of Caste Calculus in India’s Polity\n",
            "==================================================\n",
            "Processing article: Manipur can be India’s next sporting hub. Financial security need not depend on politics\n",
            "==================================================\n",
            "Processing article: Why Modi keeps pushing India to the brink of war with Pakistan\n",
            "==================================================\n",
            "Processing article: Politics latest: Indian workers exempt from UK's 'jobs tax' under new trade deal\n",
            "==================================================\n",
            "Done. Crawled latest news data saved to results\n",
            "Processing article 1/5: Mint Primer: Can Bangladesh afford to mix politics...\n",
            "  Summarizing chunk 1/5...\n",
            "  Summarizing chunk 2/5...\n",
            "  Summarizing chunk 3/5...\n",
            "  Summarizing chunk 4/5...\n",
            "  Summarizing chunk 5/5...\n",
            "  Analyzing sentiment...\n",
            "  ✓ Article 1 processed successfully\n",
            "Processing article 2/5: Parsa Venkateshwar Rao Jr | The Conundrum of Caste...\n",
            "  Summarizing chunk 1/7...\n",
            "  Summarizing chunk 2/7...\n",
            "  Summarizing chunk 3/7...\n",
            "  Summarizing chunk 4/7...\n",
            "  Summarizing chunk 5/7...\n",
            "  Summarizing chunk 6/7...\n",
            "  Summarizing chunk 7/7...\n",
            "  Analyzing sentiment...\n",
            "  ✓ Article 2 processed successfully\n",
            "Processing article 3/5: Manipur can be India’s next sporting hub. Financia...\n",
            "  Summarizing chunk 1/8...\n",
            "  Summarizing chunk 2/8...\n",
            "  Summarizing chunk 3/8...\n",
            "  Summarizing chunk 4/8...\n",
            "  Summarizing chunk 5/8...\n",
            "  Summarizing chunk 6/8...\n",
            "  Summarizing chunk 7/8...\n",
            "  Summarizing chunk 8/8...\n",
            "  Analyzing sentiment...\n",
            "  ✓ Article 3 processed successfully\n",
            "Processing article 4/5: Why Modi keeps pushing India to the brink of war w...\n",
            "  Summarizing chunk 1/7...\n",
            "  Summarizing chunk 2/7...\n",
            "  Summarizing chunk 3/7...\n",
            "  Summarizing chunk 4/7...\n",
            "  Summarizing chunk 5/7...\n",
            "  Summarizing chunk 6/7...\n",
            "  Summarizing chunk 7/7...\n",
            "  Analyzing sentiment...\n",
            "  ✓ Article 4 processed successfully\n",
            "Processing article 5/5: Politics latest: Senior Tory defends confronting f...\n",
            "  Summarizing chunk 1/3...\n",
            "  Summarizing chunk 2/3...\n",
            "  Summarizing chunk 3/3...\n",
            "  Analyzing sentiment...\n",
            "  ✓ Article 5 processed successfully\n",
            "\n",
            "Processing complete. Successfully processed 5 articles.\n",
            "INFO:     122.172.81.119:0 - \"POST /analyze/ HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "from fastapi import FastAPI, Request\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "app = FastAPI()\n",
        "\n",
        "@app.post(\"/analyze/\")\n",
        "async def process_topic(request: Request):\n",
        "    data = await request.json()\n",
        "    topic = data.get(\"topic\", \"\")\n",
        "\n",
        "    if not topic:\n",
        "        return {\"error\": \"No topic provided.\"}\n",
        "\n",
        "    fetch_data(topic)\n",
        "    inter = process_articles()\n",
        "\n",
        "    return {\"topic\": topic, \"results\": inter}\n",
        "\n",
        "# Launch ngrok and run server\n",
        "public_url = ngrok.connect(8000, url=\"mudfish-glorious-jackal.ngrok-free.app\")\n",
        "print(f\"Public URL: {public_url}/analyze/\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d4HsgNtRK-ZR"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}