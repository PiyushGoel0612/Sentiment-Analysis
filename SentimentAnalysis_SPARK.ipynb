{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install newspaper3k lxml_html_clean fastapi uvicorn nest-asyncio pyngrok transformers torch newspaper3k nltk &> /dev/null\n",
        "\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.4.0/spark-3.4.0-bin-hadoop3.tgz\n",
        "!tar xf spark-3.4.0-bin-hadoop3.tgz\n",
        "!pip install -q findspark pyspark"
      ],
      "metadata": {
        "id": "-MmrEGoPj0aG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ngrok config add-authtoken 2xmOnusF7Uhy7I9ChbKLH47TOzb_635DbfWmkg2yRmCY1Vmk"
      ],
      "metadata": {
        "id": "5lGkKMJYj9-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "global results_df\n",
        "results_df = None"
      ],
      "metadata": {
        "id": "mPbk_gBf4sT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "import time\n",
        "import re\n",
        "from datetime import datetime\n",
        "from urllib.parse import urlparse\n",
        "import torch\n",
        "from transformers import pipeline\n",
        "from newspaper import Article\n",
        "from fastapi import FastAPI, Request\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.4.0-bin-hadoop3\"\n",
        "\n",
        "import findspark\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import col, udf\n",
        "from pyspark.sql.types import StringType, StructType, StructField, ArrayType\n",
        "import json\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"SparkSentimentAnalysis\") \\\n",
        "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
        "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(f\"Spark initialized with {spark.sparkContext.defaultParallelism} cores\")\n",
        "\n",
        "API_KEY = \"03797cb80667beed8ea1bc74341941d5\"\n",
        "GNEWS_API_URL = \"https://gnews.io/api/v4/search\"\n",
        "\n",
        "device = 0 if torch.cuda.is_available() else -1\n",
        "print(f\"Using device: {'GPU' if device == 0 else 'CPU'}\")\n",
        "\n",
        "try:\n",
        "    summarizer = pipeline(\"summarization\",\n",
        "                         model=\"facebook/bart-large-cnn\",\n",
        "                         device=device)\n",
        "    sentiment_analyzer = pipeline(\"text-classification\",\n",
        "                         model=\"tabularisai/multilingual-sentiment-analysis\",\n",
        "                         device=device)\n",
        "    print(\"✓ ML models loaded successfully\")\n",
        "except Exception as e:\n",
        "    print(f\"Error loading models: {e}\")\n",
        "    summarizer = pipeline(\"summarization\", device=device)\n",
        "    sentiment_analyzer = pipeline(\"sentiment-analysis\", device=device)\n",
        "\n",
        "def clean_text_spark(text):\n",
        "    \"\"\"Clean text for Spark processing\"\"\"\n",
        "    if not text:\n",
        "        return \"\"\n",
        "\n",
        "    text = re.sub(r'[^\\x20-\\x7E\\n\\r\\t]', ' ', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text.strip()\n",
        "\n",
        "    lines = text.split('\\n')\n",
        "    cleaned = []\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        if len(line) > 5 and not line.startswith('ADVERTISEMENT'):\n",
        "            cleaned.append(line)\n",
        "\n",
        "    return ' '.join(cleaned)\n",
        "\n",
        "def extract_article_content(url):\n",
        "    \"\"\"Extract article content using newspaper3k - Spark compatible\"\"\"\n",
        "    try:\n",
        "        article = Article(url)\n",
        "        article.download()\n",
        "        article.parse()\n",
        "\n",
        "        if not article.text:\n",
        "            return None\n",
        "\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': article.title or \"No Title\",\n",
        "            'date': article.publish_date.strftime(\"%Y-%m-%d\") if article.publish_date else \"Unknown\",\n",
        "            'source': urlparse(url).netloc,\n",
        "            'text': clean_text_spark(article.text),\n",
        "            'status': 'success'\n",
        "        }\n",
        "    except Exception as e:\n",
        "        return {\n",
        "            'url': url,\n",
        "            'title': \"Extraction Failed\",\n",
        "            'date': \"Unknown\",\n",
        "            'source': urlparse(url).netloc if url else \"Unknown\",\n",
        "            'text': \"\",\n",
        "            'status': f'error: {str(e)[:100]}'\n",
        "        }\n",
        "\n",
        "clean_text_udf = udf(clean_text_spark, StringType())\n",
        "extract_article_udf = udf(extract_article_content, StringType())\n",
        "\n",
        "def get_gnews_articles(topic, max_articles=20):\n",
        "    \"\"\"Fetch articles from GNews API with increased limit for Spark processing\"\"\"\n",
        "    params = {\n",
        "        \"q\": topic,\n",
        "        \"lang\": \"en\",\n",
        "        \"max\": max_articles,\n",
        "        \"token\": API_KEY\n",
        "    }\n",
        "\n",
        "    try:\n",
        "        response = requests.get(GNEWS_API_URL, params=params)\n",
        "        response.raise_for_status()\n",
        "        articles = response.json().get(\"articles\", [])\n",
        "\n",
        "        results = []\n",
        "        for item in articles:\n",
        "            results.append({\n",
        "                'title': item.get(\"title\", \"\"),\n",
        "                'url': item.get(\"url\", \"\"),\n",
        "                'publishedAt': item.get(\"publishedAt\", \"\"),\n",
        "                'source': item.get(\"source\", {}).get(\"name\", \"\")\n",
        "            })\n",
        "\n",
        "        print(f\"✓ Found {len(results)} articles for topic '{topic}'\")\n",
        "        return results\n",
        "    except Exception as e:\n",
        "        print(f\"✗ GNews API search failed: {e}\")\n",
        "        return []\n",
        "\n",
        "def process_articles_with_spark(articles_data):\n",
        "    \"\"\"Process articles using Spark for distributed extraction\"\"\"\n",
        "    if not articles_data:\n",
        "        return []\n",
        "\n",
        "    print(f\"🚀 Processing {len(articles_data)} articles with Spark...\")\n",
        "\n",
        "    articles_df = spark.createDataFrame(articles_data)\n",
        "    print(f\"✓ Created Spark DataFrame with {articles_df.count()} articles\")\n",
        "\n",
        "    def extract_single_article(row):\n",
        "        return extract_article_content(row['url'])\n",
        "\n",
        "    articles_rdd = articles_df.rdd\n",
        "    extracted_rdd = articles_rdd.map(extract_single_article)\n",
        "\n",
        "    successful_articles = extracted_rdd.filter(lambda x: x and x['status'] == 'success' and len(x['text']) > 50)\n",
        "\n",
        "    processed_articles = successful_articles.collect()\n",
        "    print(f\"✓ Successfully extracted {len(processed_articles)} articles using Spark\")\n",
        "\n",
        "    return processed_articles\n",
        "\n",
        "def safe_summarize_batch(articles):\n",
        "    \"\"\"Batch summarization for better performance\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for article in articles:\n",
        "        try:\n",
        "            text = article['text']\n",
        "            if len(text) < 20:\n",
        "                summary = \"Content too short to summarize.\"\n",
        "            else:\n",
        "                max_length = min(10000, len(text))\n",
        "                truncated_text = text[:max_length]\n",
        "\n",
        "                summary_result = summarizer(\n",
        "                    truncated_text,\n",
        "                    max_length=120,\n",
        "                    min_length=30,\n",
        "                    do_sample=False,\n",
        "                    truncation=True\n",
        "                )\n",
        "\n",
        "                summary = summary_result[0]['summary_text']\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Summarization failed for article: {e}\")\n",
        "            summary = \"Summarization failed.\"\n",
        "\n",
        "        results.append({\n",
        "            **article,\n",
        "            'summary': summary\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "def safe_sentiment_batch(articles):\n",
        "    \"\"\"Batch sentiment analysis for better performance\"\"\"\n",
        "    results = []\n",
        "\n",
        "    for article in articles:\n",
        "        try:\n",
        "            text = article['text']\n",
        "\n",
        "            if len(text) < 10:\n",
        "                sentiment = {'label': 'NEUTRAL', 'score': 0.5}\n",
        "            else:\n",
        "                sentiment_result = sentiment_analyzer(text, truncation=True)\n",
        "                sentiment = sentiment_result[0]\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Sentiment analysis failed for article: {e}\")\n",
        "            sentiment = {'label': 'NEUTRAL', 'score': 0.5}\n",
        "\n",
        "        results.append({\n",
        "            **article,\n",
        "            'sentiment': sentiment\n",
        "        })\n",
        "\n",
        "    return results\n",
        "\n",
        "def process_complete_pipeline(topic, max_articles=20):\n",
        "    \"\"\"Complete processing pipeline using Spark + ML models\"\"\"\n",
        "    print(f\"🔍 Starting analysis for topic: '{topic}'\")\n",
        "\n",
        "    articles_data = get_gnews_articles(topic, max_articles)\n",
        "    if not articles_data:\n",
        "        return []\n",
        "\n",
        "    extracted_articles = process_articles_with_spark(articles_data)\n",
        "    if not extracted_articles:\n",
        "        print(\"No articles successfully extracted\")\n",
        "        return []\n",
        "\n",
        "    print(f\"🤖 Running ML inference on {len(extracted_articles)} articles...\")\n",
        "\n",
        "    summarized_articles = safe_summarize_batch(extracted_articles)\n",
        "    final_results = safe_sentiment_batch(summarized_articles)\n",
        "\n",
        "    sentiment_data = []\n",
        "    for article in final_results:\n",
        "        sentiment_data.append({\n",
        "            'title': article['title'],\n",
        "            'url': article['url'],\n",
        "            'sentiment_label': article['sentiment']['label'],\n",
        "            'sentiment_score': float(article['sentiment']['score'])\n",
        "        })\n",
        "\n",
        "    if sentiment_data:\n",
        "        sentiment_df = spark.createDataFrame(sentiment_data)\n",
        "\n",
        "        total_articles = sentiment_df.count()\n",
        "        avg_sentiment_score = sentiment_df.agg({'sentiment_score': 'avg'}).collect()[0][0]\n",
        "\n",
        "        sentiment_distribution = sentiment_df.groupBy('sentiment_label').count().collect()\n",
        "        sentiment_stats = {row['sentiment_label']: row['count'] for row in sentiment_distribution}\n",
        "    else:\n",
        "        total_articles = 0\n",
        "        avg_sentiment_score = 0.0\n",
        "        sentiment_stats = {}\n",
        "\n",
        "    print(f\"✅ Analysis complete!\")\n",
        "    print(f\"   📊 Total articles processed: {total_articles}\")\n",
        "    print(f\"   🎭 Average sentiment score: {avg_sentiment_score:.3f}\")\n",
        "    print(f\"   📈 Sentiment distribution: {sentiment_stats}\")\n",
        "\n",
        "    return {\n",
        "        'articles': final_results,\n",
        "        'analytics': {\n",
        "            'total_articles': total_articles,\n",
        "            'avg_sentiment_score': float(avg_sentiment_score),\n",
        "            'sentiment_distribution': sentiment_stats,\n",
        "            'processing_method': 'Spark + Transformers Hybrid'\n",
        "        }\n",
        "    }"
      ],
      "metadata": {
        "id": "2gprlm6kJDLC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "app = FastAPI(title=\"Spark-Powered Sentiment Analysis\", version=\"2.0\")\n",
        "\n",
        "@app.post(\"/analyze/\")\n",
        "async def analyze_topic(request: Request):\n",
        "    \"\"\"Analyze sentiment for a given topic using Spark + ML\"\"\"\n",
        "    try:\n",
        "        data = await request.json()\n",
        "        topic = data.get(\"topic\", \"\")\n",
        "        max_articles = data.get(\"max_articles\", 20)\n",
        "\n",
        "        if not topic:\n",
        "            return {\"error\": \"No topic provided\"}\n",
        "\n",
        "        # Process using Spark pipeline\n",
        "        results = process_complete_pipeline(topic, max_articles)\n",
        "\n",
        "        if not results:\n",
        "            print(\"NO ARTICLES PROCESSED\")\n",
        "            return {\"error\": \"No articles could be processed for this topic\"}\n",
        "\n",
        "        return {\n",
        "            \"topic\": topic,\n",
        "            \"timestamp\": datetime.now().isoformat(),\n",
        "            \"results\": results['articles'],\n",
        "            \"analytics\": results['analytics']\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"PROCESSING FAILED\", e)\n",
        "        return {\"error\": f\"Processing failed: {str(e)}\"}\n",
        "\n",
        "@app.get(\"/health/\")\n",
        "async def health_check():\n",
        "    \"\"\"Health check endpoint\"\"\"\n",
        "    return {\n",
        "        \"status\": \"healthy\",\n",
        "        \"spark_status\": \"running\" if spark else \"not available\",\n",
        "        \"ml_models\": \"loaded\",\n",
        "        \"timestamp\": datetime.now().isoformat()\n",
        "    }\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "    \"\"\"Root endpoint with API info\"\"\"\n",
        "    return {\n",
        "        \"message\": \"Spark-Powered Sentiment Analysis API\",\n",
        "        \"version\": \"2.0\",\n",
        "        \"endpoints\": {\n",
        "            \"analyze\": \"POST /analyze/ - Analyze sentiment for a topic\",\n",
        "            \"health\": \"GET /health/ - Health check\"\n",
        "        },\n",
        "        \"features\": [\n",
        "            \"Apache Spark for distributed processing\",\n",
        "            \"Hugging Face transformers for ML\",\n",
        "            \"Parallel article extraction\",\n",
        "            \"Batch ML inference\",\n",
        "            \"Real-time analytics\"\n",
        "        ]\n",
        "    }\n",
        "\n",
        "# Launch server\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"🚀 Starting Spark-powered Sentiment Analysis Server...\")\n",
        "\n",
        "    # Launch ngrok tunnel\n",
        "    public_url = ngrok.connect(8000, url=\"mudfish-glorious-jackal.ngrok-free.app\")\n",
        "    print(f\"🌐 Public URL: {public_url}\")\n",
        "    print(f\"📡 API endpoint: {public_url}/analyze/\")\n",
        "    print(f\"❤️  Health check: {public_url}/health/\")\n",
        "\n",
        "    # Apply nest_asyncio for Colab compatibility\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Run the server\n",
        "    uvicorn.run(app, host=\"0.0.0.0\", port=8000, log_level=\"info\")"
      ],
      "metadata": {
        "id": "rCj6WVdCovlx",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "p_0VopKc591-"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}